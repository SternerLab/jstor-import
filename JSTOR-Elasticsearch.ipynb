{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook was written for Python 2.7, and requires the following packages:\n",
    "- `lxml==4.0.0`\n",
    "- `elasticsearch==5.4.0`\n",
    "- `xmltodict==0.11.0h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Define environment variables:\n",
    "- `DATASET_DIR`:\n",
    "Path to the dataset directory.\n",
    "\n",
    "- `ES_INDEX_NAME`:\n",
    "The elasticsearch index where you want the documents to be uploaded.\n",
    "\n",
    "- `ES_DOCUMENT_TYPE`:\n",
    "The document type you want to use for each uploaded document.\n",
    "\n",
    "- `ES_HOST`:\n",
    "The connection string for accessing Elasticsearch instance. The format is as follows:\n",
    "```\n",
    "http://<username>:<password>@<host>:<port>\n",
    "```\n",
    "Example: `http://user:pass@localhost:9200`. If the ES instance doesn't require authentication, you can specify `http://<host>:<port>` as the connection string. If `ES_HOST = ''`, `http://localhost:9200` will be used as the connection string.\n",
    "- `ES_CREATE_INDEX`:\n",
    "Create index if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR      = 'receipt-id-430111-jcodes-klmnop-part-001/'\n",
    "ES_INDEX_NAME    = 'jstor'\n",
    "ES_DOCUMENT_TYPE = 'article'\n",
    "ES_HOST          = 'http://username:password@localhost:9200'\n",
    "ES_CREATE_INDEX  = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging\n",
    "\n",
    "To get realtime logs of processed directories, set ``'dataset'`` logger's level to `logging.INFO` or `logging.DEBUG`.  \n",
    "  \n",
    "  \n",
    "**Note**: `'elasticsearch'` logger generates high amounts of debug log statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "## Set to INFO/DEBUG to get realtime logs of processed directories.\n",
    "# logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting Implementation\n",
    "\n",
    "### UTF8 encoding\n",
    "\n",
    "*NOTE: This step is not needed for Python 3 and above.*  \n",
    "\n",
    "Ensure all strings use `utf-8` encoding by default, else you may run into `ordinal not in range` errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('UTF8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xmltodict\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "class XMLParser(object):\n",
    "    def _process_if_avail(self, d, key, func):\n",
    "        try:\n",
    "            value = d.pop(key)\n",
    "        except KeyError:\n",
    "            return\n",
    "        else:\n",
    "            if value is not None:\n",
    "                new_value = func(value)\n",
    "                if new_value is not None:\n",
    "                    d[key] = new_value\n",
    "\n",
    "    def _process__article_meta__contrib_group(self, contrib_groups):\n",
    "        if not isinstance(contrib_groups, list):\n",
    "            contrib_groups = [contrib_groups]\n",
    "\n",
    "        for contrib_group in contrib_groups:\n",
    "            if not isinstance(contrib_group['contrib'], list):\n",
    "                contrib_group['contrib'] = [contrib_group['contrib']]\n",
    "\n",
    "            try:\n",
    "                if isinstance(contrib_group['aff'], basestring):\n",
    "                    contrib_group['aff'] = [contrib_group['aff']]\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "            for i, contrib in enumerate(contrib_group['contrib']):\n",
    "                try:\n",
    "                    name = contrib['string-name']\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                else:\n",
    "                    if isinstance(name, dict):\n",
    "                        name = u'{} {}'.format(name.get('given-names', ''), name.get('surname', ''))\n",
    "                    contrib_group['contrib'][i]['string-name'] = name\n",
    "                try:\n",
    "                    if isinstance(contrib['aff'], basestring):\n",
    "                        contrib['aff'] = [contrib['aff']]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "        return contrib_groups\n",
    "\n",
    "    def _process__article_meta__pub_date(self, pub_dates):\n",
    "        if isinstance(pub_dates, dict):\n",
    "            if isinstance(pub_dates['year'], list):\n",
    "                pub_date_list = []\n",
    "                for i in xrange(len(pub_dates['year'])):\n",
    "                    date = { }\n",
    "                    if 'day' in pub_dates.keys(): date['day'] = pub_dates['day'][i]\n",
    "                    if 'month' in pub_dates.keys(): date['month'] = pub_dates['month'][i]\n",
    "                    if 'year' in pub_dates.keys(): date['year'] = pub_dates['year'][i]\n",
    "                    pub_date_list.append(date)\n",
    "\n",
    "                pub_dates = pub_date_list\n",
    "            else:\n",
    "                pub_dates = [pub_dates]\n",
    "\n",
    "        for pub_date in pub_dates:\n",
    "            self._process_if_avail(pub_date, 'day',   int)\n",
    "            self._process_if_avail(pub_date, 'month', int)\n",
    "            self._process_if_avail(pub_date, 'year',  int)\n",
    "\n",
    "        return pub_dates\n",
    "\n",
    "    def _process__article_meta(self, article_meta):\n",
    "        _make_str = lambda d: d.get('#text', None) if isinstance(d, dict) else d\n",
    "\n",
    "        # Remove duplicates\n",
    "        for key in ('issue-id', 'issue', 'volume', 'pub-date'):\n",
    "            if isinstance(article_meta.get(key, ''), list):\n",
    "                if article_meta[key][0] == article_meta[key][1]:\n",
    "                    article_meta[key] = article_meta[key][0]\n",
    "\n",
    "        self._process_if_avail(article_meta, 'contrib-group', self._process__article_meta__contrib_group)\n",
    "        self._process_if_avail(article_meta, 'pub-date', self._process__article_meta__pub_date)\n",
    "        self._process_if_avail(article_meta, 'issue-id', _make_str)\n",
    "        self._process_if_avail(article_meta, 'issue', _make_str)\n",
    "        self._process_if_avail(article_meta, 'volume', _make_str)\n",
    "        try:\n",
    "            self._process_if_avail(article_meta['title-group'], 'article-title', _make_str)\n",
    "        except KeyError:\n",
    "            pass\n",
    "        return article_meta\n",
    "\n",
    "    def _process__journal_meta(self, journal_meta):\n",
    "        journal_title_str = lambda journal_title: journal_title['#text'] if isinstance(journal_title, dict) else journal_title\n",
    "        self._process_if_avail(journal_meta['journal-title-group'], 'journal-title', journal_title_str)\n",
    "        return journal_meta\n",
    "\n",
    "    def _get_parse_postprocessor(self, article_et):\n",
    "        etree_str = lambda e: etree.tostring(e, encoding='utf-8', method='text').strip()\n",
    "        self._seen1 = False\n",
    "        self._seen2 = False\n",
    "        def postprocessor(path, key, value):\n",
    "            xpath = '/'.join((path[i][0] for i in xrange(1, len(path))))\n",
    "\n",
    "            if value is None:\n",
    "                return None, None\n",
    "\n",
    "            if key in set((\n",
    "                '@xmlns:xsi',\n",
    "                '@xlink:type',\n",
    "                '@ext-link-type',\n",
    "                '@content-type',\n",
    "                '@dtd-version',\n",
    "                '@xmlns:oasis',\n",
    "                '@xmlns:xlink',\n",
    "                '@xmlns:mml',\n",
    "                '@xlink:role',\n",
    "                '@xlink:title',\n",
    "                )):\n",
    "                return None, None\n",
    "\n",
    "            if key == 'email':\n",
    "                if isinstance(value, dict):\n",
    "                    value = value['#text']\n",
    "                return key, value\n",
    "\n",
    "            if key in set((\n",
    "                'page-count',\n",
    "                'ref-count',\n",
    "                'fig-count',\n",
    "                'equation-count',\n",
    "                'table-count',\n",
    "                )):\n",
    "                return key, int(value['@count'])\n",
    "\n",
    "            if xpath in set((\n",
    "                'front/article-meta/kwd-group/x',\n",
    "                'front/article-meta/contrib-group/x',\n",
    "                'front/article-meta/contrib-group/xref',\n",
    "                'front/article-meta/contrib-group/contrib/x',\n",
    "                'front/article-meta/contrib-group/contrib/xref',\n",
    "                )):\n",
    "                return None, None\n",
    "\n",
    "            if not isinstance(value, basestring):\n",
    "                if xpath in set((\n",
    "                    'front/article-meta/abstract',\n",
    "                    'front/article-meta/trans-abstract',\n",
    "                    'front/article-meta/title-group/article-title',\n",
    "                    'front/article-meta/title-group/subtitle',\n",
    "                    'front/article-meta/title-group/trans-title-group/trans-title',\n",
    "                    'front/article-meta/author-notes',\n",
    "                    'front/article-meta/bio',\n",
    "                    'back/ack',\n",
    "                    'body',\n",
    "                    )):\n",
    "                    elements = article_et.xpath(xpath)\n",
    "                    value = etree_str(elements[0]).strip()\n",
    "                    return key, value\n",
    "\n",
    "                elif xpath in set((\n",
    "                    'back/app-group/app',\n",
    "                    'back/fn-group',\n",
    "                    'back/sec',\n",
    "                    'front/notes',\n",
    "                    'front/article-meta/kwd-group/kwd',\n",
    "                    'front/article-meta/contrib-group/bio',\n",
    "                    'front/article-meta/contrib-group/fn',\n",
    "                    )):\n",
    "                    element = article_et.xpath(xpath + '[count(*)>0]')[0]\n",
    "                    value = etree_str(element)\n",
    "                    element.getparent().remove(element)\n",
    "                    return key, value\n",
    "\n",
    "                elif xpath == 'back/ref-list/ref/mixed-citation':\n",
    "                    refid = path[3][1]['id']\n",
    "                    value = ' '.join(article_et.xpath(\"back/ref-list/ref[@id='{}']/mixed-citation/text()\".format(refid))).strip()\n",
    "                    return key, value\n",
    "\n",
    "                elif isinstance(value, dict) and xpath in set((\n",
    "                    'front/article-meta/contrib-group/aff',\n",
    "                    'front/article-meta/contrib-group/contrib/aff',\n",
    "                    )):\n",
    "                    try:\n",
    "                        aff = article_et.xpath(xpath + '[count(*)>0]')[0]\n",
    "                        value = etree_str(aff)\n",
    "                        aff.getparent().remove(aff)\n",
    "                    except IndexError:\n",
    "                        value = value['#text']\n",
    "                    return key, value\n",
    "\n",
    "            return key, value\n",
    "\n",
    "        return postprocessor\n",
    "\n",
    "    def parse(self, xml_path):\n",
    "        parser = etree.XMLParser(recover=True)\n",
    "        article_et = etree.parse(xml_path, parser=parser).getroot()\n",
    "\n",
    "        with open(xml_path, 'r') as fh:\n",
    "            article = xmltodict.parse(fh.read(), postprocessor=self._get_parse_postprocessor(article_et))['article']\n",
    "\n",
    "        article['front']['journal-meta'] = self._process__journal_meta(article['front']['journal-meta'])\n",
    "        article['front']['article-meta'] = self._process__article_meta(article['front']['article-meta'])\n",
    "\n",
    "        self._process_if_avail(article['front'], 'notes', lambda v: v if isinstance(v, list) else [v])\n",
    "        self._process_if_avail(article.get('back', {}), 'sec', lambda v: v if isinstance(v, list) else [v])\n",
    "\n",
    "        return article\n",
    "\n",
    "class TXTParser(object):\n",
    "    def parse(self, txt_path):\n",
    "        txt_root = ET.parse(txt_path).getroot()\n",
    "        if txt_root.tag == 'plain_text':\n",
    "            page_seq = [(p.attrib['sequence'], p.text) for p in list(txt_root)]\n",
    "            page_seq.sort(key=lambda x: x[0])\n",
    "            plain_text_pages = [p for s, p in page_seq]\n",
    "            return {'plain_text': plain_text_pages}\n",
    "        elif txt_root.tag == 'body':\n",
    "            return {'body': ET.tostring(txt_root, encoding='utf-8', method='text')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset article actions generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_actions(dataset_dir, index, document_type):\n",
    "    logger = logging.getLogger()\n",
    "    abs_dataset_dir = os.path.abspath(os.path.expanduser(dataset_dir))\n",
    "    xmlparser = XMLParser()\n",
    "    txtparser = TXTParser()\n",
    "    doc_id = 0\n",
    "    for (dpath, dnames, fnames) in os.walk(abs_dataset_dir, topdown=False):\n",
    "        if not fnames:\n",
    "            continue\n",
    "\n",
    "        logger.info('Processing %s' % dpath)\n",
    "        document = {}\n",
    "\n",
    "        xml_files = [p for p in fnames if p.lower().endswith('.xml')]\n",
    "        if xml_files:\n",
    "            if len(xml_files) > 1:\n",
    "                logger.warning('Multiple xml files found in %s. Ignoring...' % dpath)\n",
    "            else:\n",
    "                document['article'] = xmlparser.parse(os.path.join(dpath, xml_files[0]))\n",
    "\n",
    "        txt_files = [p for p in fnames if p.lower().endswith('.txt')]\n",
    "        if txt_files:\n",
    "            if len(txt_files) > 1:\n",
    "                logger.warning('Multiple txt files found in %s. Ignoring...' % dpath)\n",
    "            else:\n",
    "                txt_path = os.path.join(dpath, txt_files[0])\n",
    "                try:\n",
    "                    document.update(txtparser.parse(txt_path))\n",
    "                except ET.ParseError:\n",
    "                    logger.warning('ERROR reading \\'%s\\'. Ignoring txt file...' % (txt_path))\n",
    "\n",
    "        if document:\n",
    "            action = {\n",
    "                '_index': index,\n",
    "                '_type': document_type,\n",
    "                '_id': doc_id,\n",
    "                '_source': document\n",
    "            }\n",
    "            doc_id += 1\n",
    "            yield action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to ES\n",
    "\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import elasticsearch.helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = elasticsearch.Elasticsearch([ES_HOST]) if ES_HOST else elasticsearch.Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Index\n",
    "\n",
    "Create index if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es.indices.delete(index=ES_INDEX_NAME)\n",
    "if ES_CREATE_INDEX:\n",
    "    es.indices.create(index=ES_INDEX_NAME, ignore=[400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Upload\n",
    "\n",
    "Start (bulk) uploading documents to Elasticsearch. Use `chunk_size` parameter to control how many documents are uploaded in one request. By default, at most `500` documents are uploaded per request.  \n",
    "\n",
    "Depending on the log level, real-time logs of processed directories may be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_generator = generate_actions(DATASET_DIR, index=ES_INDEX_NAME, document_type=ES_DOCUMENT_TYPE)\n",
    "\n",
    "# elasticsearch.helpers.bulk(es, action_generator, chunk_size=100)\n",
    "elasticsearch.helpers.bulk(es, action_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "The following command just gets the count of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.count(index=ES_INDEX_NAME, doc_type=ES_DOCUMENT_TYPE)['count']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
